{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing @ 36c3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these 'cells' contains code which can be executed by selecting the cell and pressing **CTRL + Enter**. If you do not have a keyboard, you can select the cell and press the **Run** button on top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there are thousands of things we could do, we will limit ourselves to a couple of simple NLP tasks. First, we will implement some of them in pure Python before moving to spaCy, a very robust and widely used NLP library. The code in this notebook is not optimized/writted for production!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from difflib import SequenceMatcher\n",
    "import matplotlib\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import gensim\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import en_core_web_sm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIY NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'She also missed her spaceship to school.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = re.findall('[a-zA-Z]{4,}', text)\n",
    "#results = re.findall('[a-zA-Z]*ed', text)\n",
    "\n",
    "for r in results:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reading and Tokenizing a Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file and skip description; change case to lowercase\n",
    "with Path('demo-data/BROWN-a1-a44-modified.txt') as corpus_file:\n",
    "    corpus = corpus_file.read_text()[350:].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return re.split('\\s+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokens = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Frequency List & Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_list(tokens):\n",
    "    return Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_list(tokens).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zipf's Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = []\n",
    "\n",
    "for i, token in enumerate(frequency_list(tokens).most_common()):\n",
    "    freqs.append([token[0], i, token[1], math.log(token[1]), len(token[0])])\n",
    "\n",
    "df = pd.DataFrame.from_records(freqs, columns=['wordform', 'rank', 'frequency', 'logfrequency', 'length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='scatter', y='logfrequency', x='rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='scatter', y='logfrequency', x='length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams(tokens, n):\n",
    "    n_grams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [n_gram for n_gram in n_grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams(tokens, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. A Very Simple N-Gram (Trigram) Language Model\n",
    "\n",
    "The goal is to predict the/a next word in a sequence of words. We will be implementing the most basic (n-gram-based) language model possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = n_grams(tokens, 3)\n",
    "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "for t0, t1, t2 in trigrams:\n",
    "    model[(t0, t1)][t2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['i', 'have']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t0t1 in model:\n",
    "    count = float(sum(model[t0t1].values()))\n",
    "    for t2 in model[t0t1]:\n",
    "        model[t0t1][t2] /= count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['and', 'we']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, starting_tokens, max_length, threshold=0.1, verbose=False):\n",
    "    \n",
    "    text = starting_tokens\n",
    "    \n",
    "    while not len(text) > max_length:\n",
    "        candidates = []\n",
    "        for token in model[tuple(text[-2:])].keys():\n",
    "            if model[tuple(text[-2:])][token] >= threshold:\n",
    "                candidates.append(token)\n",
    "        \n",
    "        try:\n",
    "            random_candidate = random.choice(candidates)              \n",
    "            text.append(random_candidate)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f'* \"{\" \".join(text)}\" could be continued with {candidates} --> {random_candidate}')\n",
    "            \n",
    "        except IndexError:\n",
    "            # There are no valid candidates anymore\n",
    "            break\n",
    "                \n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(model, ['the', 'person'], 5, 0.05, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Crude Spelling Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Vocabulary Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_match(vocabulary, search):\n",
    "    best_match = [0, '']\n",
    "    \n",
    "    for word in vocabulary:\n",
    "        ratio = SequenceMatcher(None, search, word).ratio()\n",
    "        if ratio > best_match[0]:\n",
    "            best_match = [ratio, word]\n",
    "            \n",
    "    return best_match\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest_match(vocabulary, 'girk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_vocab_approach(text):\n",
    "    wrong = tokenize(text.lower())\n",
    "    corrected = []\n",
    "\n",
    "    for token in wrong:\n",
    "        if token in vocabulary:\n",
    "            corrected.append(token)\n",
    "        else:\n",
    "            corrected.append(find_closest_match(vocabulary, token)[1])\n",
    "    \n",
    "    return ' '.join(corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Vocabulary & Model Appproach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_vocabmodel_approach(model, text):\n",
    "    wrong = tokenize(text.lower())\n",
    "    corrected = []\n",
    "\n",
    "    for i, token in enumerate(wrong):\n",
    "        if token in vocabulary:\n",
    "            corrected.append(token)\n",
    "        else:\n",
    "            candidates = list(model[(wrong[i-2], wrong[i-1])])\n",
    "            if len(candidates) > 0:\n",
    "                corrected.append(find_closest_match(candidates, token)[1])\n",
    "            else:\n",
    "                corrected.append('X')\n",
    "                    \n",
    "    return ' '.join(corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong = 'The man whp had a beautiful cat.'\n",
    "\n",
    "print(f'Approach A: {correct_vocab_approach(wrong)}')\n",
    "print(f'Approach B: {correct_vocabmodel_approach(model, wrong)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = nlp('She missed her spaceship to school.')\n",
    "doc = nlp(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Navigating the Doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, spaCy runs a number of taggers and parsers. Hence, our *doc* now contains much more information than just the raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in sentences[0]:\n",
    "    print(f'{token.text} ({token.lemma_}) is a {token.pos_}/{token.tag_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in example_sentence:\n",
    "    print(token, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Depedency Parsing / Universal Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in example_sentence:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(example_sentence, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization and Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "            'She missed her spaceship to school.',\n",
    "            'The spaceship usually is a little bit late.'\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_sentences = []\n",
    "for sentence in sentences:\n",
    "    w2v_sentences.append([token.text for token in sentence])\n",
    "    \n",
    "w2v_sentences[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(w2v_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similarity('the', 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with a pre-trained model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-ruscorpora-300') # Russian National Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar(positive='город_NOUN') # город = city"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
